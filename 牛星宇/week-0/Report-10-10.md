# Pytorch

## 一、Pytorch 相关

### （一）神经网络基础结构
神经网络的层级结构通常包括**输入层**、**隐藏层**和**输出层**，每一层都由若干个神经元（或称为节点）组成，其中每个节点可看作是一个感知机模型的实例化体现。

### （二）常用神经网络模型
在深度学习领域，有多种经典且常用的神经网络模型，例如：
- **NN（神经网络）**：是深度学习的基础模型，通过多层神经元的连接来学习数据特征。
- **RNN（循环神经网络）**：擅长处理序列数据，能捕捉数据中的时序依赖关系，常用于自然语言处理、语音识别等任务。
- **CNN（卷积神经网络）**：在计算机视觉领域表现出色，利用卷积操作提取图像的局部特征，如边缘、纹理等。
- **GNN（图神经网络）**：专为处理图结构数据而设计，可有效挖掘图中节点与节点之间的关系，在社交网络分析、推荐系统等场景中应用广泛。

### （三）神经网络的特征提取机制
输入层的数据会被分配相应的权重，感知机模型带有偏置项，对输入数据与权重进行求和运算后得出初步结果。接着，利用损失函数来比较模型预测结果与真实结果的差异，然后通过自我更新（如反向传播算法）来优化模型的参数（权重和偏置等），使得模型能更精准地提取数据特征并进行预测。

### （四）`torch.nn.Linear`类详解
`torch.nn.Linear`类是 PyTorch 中用于实现线性变换的核心类，在构建神经网络的线性层时不可或缺。
- **类定义**：`CLASS torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`，它对输入数据执行线性变换，数学公式为\( y = xA^T + b \)。其中，\( x \)是输入数据（张量形式），\( A \)是权重矩阵，\( b \)是偏置向量，\( y \)是经过线性变换后的输出。
- **参数说明**：
  - `in_features`：输入样本的特征数量，也就是输入数据的维度大小。
  - `out_features`：输出样本的特征数量，决定了线性变换后输出结果的维度。
  - `bias`：布尔类型参数，若设置为`True`，该线性层会学习一个加性偏置；若为`False`，则不学习偏置，其默认值为`True`。
- **示例关联**：结合简单神经元模型（输入\( x_1,x_2,x_3 \)经权重\( w_1,w_2,w_3 \)和偏置等运算得到输出\( y \)）以及典型的神经网络结构（输入层、隐藏层、输出层依次连接）来看，`in_features`对应输入层或者前一层网络的特征数量，`out_features`对应后一层（如隐藏层或者输出层）的神经元数量等，用于保障线性变换过程中输入输出维度的匹配，确保数据能在网络各层间顺利传递。

### （五）线性变换的作用讲解
在神经网络里，线性变换是极为基础且关键的操作，`torch.nn.Linear`类为我们便捷地实现了这一操作。比如，当我们要构建一个从具有3个特征的输入层到包含4个神经元的隐藏层的线性连接时，将`in_features`设置为3，`out_features`设置为4，该类就会依据输入数据计算出到这4个神经元的线性输出。之后，再结合激活函数（如ReLU、Sigmoid等），就能让神经网络具备对复杂数据进行表示与学习的能力，去挖掘数据中更抽象、更有价值的特征。而偏置`bias`的存在，能让模型的拟合能力更灵活，可以更好地适配不同的数据分布情况；要是数据本身的特性不需要偏置产生影响，我们也可以将`bias`设为`False`。

### （六）神经网络监督训练：梯度下降算法
在神经网络的监督训练过程中，**梯度下降算法**是优化模型参数的核心方法。其基本思想是：计算损失函数关于模型参数（如`torch.nn.Linear`层的权重和偏置）的梯度，然后沿着梯度的反方向更新参数，使得损失函数逐渐减小，从而让模型的预测结果越来越接近真实值。

具体来说，假设损失函数为\( L(\theta) \)（\( \theta \)表示模型参数），梯度下降的参数更新公式为：\( \theta_{new} = \theta_{old} - \eta \cdot \nabla L(\theta_{old}) \)，其中\( \eta \)是学习率，控制着参数更新的步长；\( \nabla L(\theta_{old}) \)是损失函数在当前参数\( \theta_{old} \)处的梯度。通过不断迭代这个过程，模型能够逐步优化，提升在训练数据上的表现，进而具备良好的泛化能力去预测未见过的数据。
